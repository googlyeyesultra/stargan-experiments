Original has convolution artifacts presumably from Conv2dTranspose.

Autoencoder ran in 10:16 on celeba. Original ran in 10:22. Probably not significant - running on shared machines.
Autoencoder results very similar but probably a tiny bit worse. Can try FID/etc. to actually evaluate.
    Certainly close enough to be worth considering.
    
Deg_1 very poor performance.
   
Upscale is somewhat better with a full run but still not ideal.
    
Bilinear and learned bilinear both perform poorly.

Degree 3 is sometimes better than original, though there are occasional spots it visibly misbehaves a little bit.]
    Basically, doesn't have checkerboard artifacts, and hews closer to original image.
    But has spots where it recolors wrong thing or doesn't change something it should.
    Original sort of does the same but it is less obvious.
    
    
NOTE THAT THE POLYNOMIAL ONES ARE USING 10 D STEPS PER G UPDATE - ORIGINAL IS USING 5
Should compare without for fairness, or run original with 10.


Adding original labels to generator has mixed results. Some are a little better, some a little worse.
Polynomials above degree 3 don't really improve anything over degree 3. Maaaaybe degree 5 is a bit better, but hard to tell.

Adding positional information seems to have fairly minor impact.
Slight autoencoder seems to have minor impact.
10 discrim cycles/gen cycle seems to be a bit better on deg_3. Very similar on baseline.
Nearest neighbor vs. bilinear seems to be fairly minor.